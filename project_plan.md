# Implementation Plan — Anthropic `SKILL.md` System (Token-Efficient, Standards-Aligned)

## Objectives (what to build)

1. A repo that stores many small, composable Skills (one per folder) using Anthropic’s `SKILL.md` standard.
2. A meta “Skill-Creation Skill” that generates new skills on demand (with tiering, examples, quality gates).
3. A lightweight discovery index (`index/skills-index.json`) and optional embeddings for routing without loading full skills.
4. CI that validates skill structure, regenerates index/embeddings, and runs tiny task evals.
5. Strict guardrails: normalize time to NIST ET; never embed secrets; keep examples tiny; progressive disclosure to minimize context use.

---

## Deliverables (artifacts the agent must produce)

* Repo scaffold:

  ```
  skills-repo/
    index/
      skills-index.json         # generated
      embeddings/               # optional vectors (generated)
    skills/
      skill-creation/
        SKILL.md
        examples/
          create-skill-minimal.txt
        resources/
          README.md
        CHANGELOG.md
    tests/
      evals_skill-creation.yaml
    tooling/
      validate_skill.py
      build_index.py
      lint_skill.py
    .github/workflows/skills-ci.yaml
    README.md
    CONTRIBUTING.md
    CODE_OF_CONDUCT.md
    LICENSE
  ```
* `skills/skill-creation/SKILL.md` (meta-skill that manufactures new skills)
* Two sample production skills (minimal but real):

  * `skills/oscal-ssp-validate/SKILL.md`
  * `skills/fedramp-poam-qc/SKILL.md`
* `index/skills-index.json` (auto-generated by `tooling/build_index.py`)
* Tiny example files (≤30 lines) for each skill under `examples/`
* Eval files: `tests/evals_<slug>.yaml` (3–5 scenarios each)
* CI workflow that runs validation, builds index/embeddings, and evals

---

## Workplan (phased for agents)

### Phase 0 — Repo bootstrap (scaffold + governance)

**Tasks**

* Create directory structure above.
* Add `README.md` explaining purpose, loading model, and progressive disclosure.
* Add `CONTRIBUTING.md`, `CODE_OF_CONDUCT.md`, `LICENSE` (Apache-2.0).
* Add `.gitignore` (Python, node, editor junk).
* Add `pyproject.toml` (if needed) for tooling scripts.

**Definition of Done (DoD)**

* Repo clones clean; no broken paths.
* `README.md` shows “how to add a skill in 60 seconds.”

---

### Phase 1 — Tooling & CI

**Tasks**

1. `tooling/validate_skill.py`

   * Enforce required metadata keys in front-matter:

     * `name, slug, description, capabilities, inputs, outputs, keywords, version, owner, license, security, links`
   * Length caps: `description ≤ 160 chars`; examples ≤ 30 lines.
   * Must have `Purpose & When-To-Use`, `Pre-Checks`, `Procedure (Tiered)`, `Decision Rules`, `Output Contract`, `Quality Gates`, `Resources`.
   * Ensure presence of token budgets per Tier (T1/T2/T3).
   * Fail on likely-secret patterns and oversized blocks.

2. `tooling/lint_skill.py`

   * Style checks: headings order, code fences closed, links valid.

3. `tooling/build_index.py`

   * Walk `skills/*/SKILL.md`, parse metadata, write `index/skills-index.json`.
   * Optional: build embeddings from `name+description+keywords` into `index/embeddings/`.
   * Sort index by `slug`; ensure deterministic output.

4. `.github/workflows/skills-ci.yaml`

   * Jobs: setup → validate → lint → build_index → run_evals → upload artifacts.
   * Fail PR if any step fails; post summary.

**DoD**

* `python tooling/validate_skill.py` returns 0.
* On push/PR, CI passes and publishes the index as an artifact.

---

### Phase 2 — Meta “Skill-Creation Skill”

**Tasks**

* Create `skills/skill-creation/SKILL.md` implementing:

  * NIST/time.gov normalization (`NOW_ET` ISO 8601, `America/New_York`).
  * Tiered creation procedure:

    * T1: generate minimal `SKILL.md` with concise metadata, one short example, token budgets.
    * T2: add 2–4 primary sources with titles + URLs + access date (`NOW_ET`), decision rules, small `resources/` links.
    * T3: emit `tests/evals_<slug>.yaml` (3–5 scenarios, budgets), `CHANGELOG.md`, and `index_entry` JSON.
  * Output contract that creates `skills/<slug>/` folder with all files.

* Add `examples/create-skill-minimal.txt` showing expected I/O (≤30 lines).

* Add `tests/evals_skill-creation.yaml` covering:

  * Generates a valid skill for a simple topic.
  * Blocks with TODO when inputs are insufficient.
  * Respects token budgets.

**DoD**

* Running the meta-skill prompt (below) yields a valid skill folder that passes validate/lint/build_index/evals.

---

### Phase 3 — Two pilot skills

**Tasks**

1. `skills/oscal-ssp-validate/SKILL.md`

   * Inputs: `ssp_path (file|url)`, `profile (string)`, `strict (bool)`
   * Outputs: `report (json)`, `summary (markdown)`
   * T1: schema validation; T2: profile checks; T3: cross-ref and rationale
   * Examples: one ≤30 lines; `tests/evals_oscal-ssp-validate.yaml` with 3–5 cases.

2. `skills/fedramp-poam-qc/SKILL.md`

   * Inputs: `poam_tsv (file)`, `naming_convention (json)`
   * Outputs: `findings (json)`, `fix_suggestions (markdown)`
   * T1: header+format sanity; T2: naming convention & dedupe; T3: cross-sheet consistency
   * Examples + evals as above.

**DoD**

* Both skills pass validation, lint, index build, and evals in CI.
* Index shows 3 entries total: `skill-creation`, `oscal-ssp-validate`, `fedramp-poam-qc`.

---

### Phase 4 — Retrieval & routing (minimal viable)

**Tasks**

* Implement simple keyword router (optional script or doc instructions):

  * Load `index/skills-index.json`.
  * Match by `keywords` OR basic embedding cosine similarity (if embeddings built).
  * Return up to **2** candidate `slug`s to load; never shotgun the entire repo.

**DoD**

* Router returns the intended skill for representative queries.
* Hard cap: ≤2 skills loaded per request.

---

## Global To-Do (copy/paste into issues)

* [ ] Initialize repo scaffold and governance files.
* [ ] Implement `tooling/validate_skill.py` (schema + budgets + size + secrets).
* [ ] Implement `tooling/lint_skill.py` (style/order/links).
* [ ] Implement `tooling/build_index.py` (metadata → JSON + embeddings optional).
* [ ] Add GitHub Actions workflow `.github/workflows/skills-ci.yaml`.
* [ ] Author `skills/skill-creation/SKILL.md` (meta-skill).
* [ ] Add meta-skill example + evals.
* [ ] Generate skill: `oscal-ssp-validate` (+ examples, resources, evals).
* [ ] Generate skill: `fedramp-poam-qc` (+ examples, resources, evals).
* [ ] Run CI and fix any failures.
* [ ] (Optional) Add simple router script or doc for runtime loading policy.
* [ ] Tag initial release `v0.1.0`.

---

## Checklists

### Authoring Checklist (per new skill)

* [ ] Front-matter includes: `name, slug, description (≤160), capabilities, inputs, outputs, keywords, version, owner, license, security, links`.
* [ ] `Purpose & When-To-Use` is explicit and short.
* [ ] `Pre-Checks` includes **NIST/time.gov normalization** to `NOW_ET` and input schema validation.
* [ ] `Procedure` is tiered (T1/T2/T3) with clear escalation.
* [ ] `Decision Rules` define ambiguity thresholds & abort conditions.
* [ ] `Output Contract` defines types + required fields.
* [ ] At least **one** example ≤30 lines (runnable or precise pseudo-code).
* [ ] `Quality Gates`: token budgets per tier; safety; auditability.
* [ ] `Resources` are **links** (no pasted long text).
* [ ] `tests/evals_<slug>.yaml` has 3–5 scenarios with budgets and pass/fail.
* [ ] `CHANGELOG.md` starts at `1.0.0`.
* [ ] `tooling/validate_skill.py` passes; `lint_skill.py` passes.
* [ ] Index updated via `tooling/build_index.py`; CI green.

### CI Checklist

* [ ] Validate → Lint → Build Index → (Optional) Build Embeddings → Run Evals
* [ ] Fail PR on any error; upload index/embeddings as artifacts
* [ ] Enforce max example length and presence of token budgets

### Security & Compliance Checklist

* [ ] No secrets or credentials embedded (scan common patterns).
* [ ] PII: “none” or clearly documented minimal handling.
* [ ] Standards citations include **title, URL, access date (`NOW_ET`)**.
* [ ] Output does not leak raw source content beyond fair use snippets.

### Routing Checklist

* [ ] `skills-index.json` contains minimal fields for discovery.
* [ ] Hard cap: load ≤2 `SKILL.md` files per request.
* [ ] (If embeddings) vectors built from `name+description+keywords` only.

---

## Agent Prompts (drop-in)

### 1) Meta-skill invocation (to create a new skill)

```
SYSTEM:
You are a surgical builder of Anthropic Skills. Conform to the repo’s validate/lint rules. Use progressive disclosure and tiny examples.

ASSISTANT RULES:
- Normalize time using NIST/time.gov semantics as ISO 8601 ET (America/New_York); store as NOW_ET.
- Keep metadata tiny; never embed secrets; cite sources with titles + URLs + access dates.
- Provide one short runnable example (≤30 lines) or precise pseudo-code.
- Include token budgets: T1≤2k, T2≤6k, T3≤12k.
- Emit index_entry JSON.

USER:
Create a new Skill.

Topic: "<TOPIC_TEXT>"
Org context (optional): <JSON or prose>
Constraints (optional): <JSON or bullets>

OUTPUT:
- skills/<slug>/SKILL.md
- skills/<slug>/examples/<slug>-example.txt
- tests/evals_<slug>.yaml
- index_entry JSON
```

### 2) Router prompt (if using LLM for routing)

```
SYSTEM:
You are a minimal router. Load only index/skills-index.json. Return the best 1–2 skill slugs based on keywords and brief description match. Do not open SKILL.md unless asked.

USER:
Route this request to skill(s):
"<USER_TASK>"
```

---

## File Specs (for the agent)

### `index/skills-index.json` (append)

```json
[
  {
    "slug": "skill-creation",
    "name": "Create a New Skill from Topic",
    "summary": "Generates a production-ready SKILL.md, examples, evals, and index entry.",
    "keywords": ["authoring","generator","skills.md","progressive-disclosure"],
    "owner": "cloud.gov OCS",
    "version": "1.0.0",
    "entry": "skills/skill-creation/SKILL.md"
  }
]
```

### `tests/evals_<slug>.yaml` (skeleton)

```yaml
meta:
  slug: <slug>
  budgets:
    tier1_tokens: 2000
    tier2_tokens: 6000
    tier3_tokens: 12000

cases:
  - name: "Happy path minimal"
    input: { ... }
    expect:
      status: pass
      has_output_fields: ["summary","report"]

  - name: "Missing required input"
    input: { ... }
    expect:
      status: fail
      reason_contains: "needs-input"
```

---

## Risks & Mitigations

* **Scope creep / token sprawl** → Progressive disclosure; enforce budgets in validator.
* **Citation rot** → Require access date `NOW_ET`; periodic doc check (weekly CI job optional).
* **Secret leakage** → Regex checks + denylist in validator; PR gate.
* **Routing misses** → Add/curate `keywords`; optional embeddings; cap loaded skills to ≤2.

---

## Success Criteria (acceptance)

* CI green on initial commit with meta-skill + 2 pilot skills.
* Index discovery returns correct skills for canonical queries.
* Each skill remains compact (≤ ~200–400 lines total text, examples ≤30 lines).
* New skills can be generated in one agent call and pass CI on first PR.

---

## Immediate Next Actions (for the agent)

1. Scaffold repo and governance files.
2. Implement `validate_skill.py`, `lint_skill.py`, `build_index.py`.
3. Add CI workflow.
4. Commit `skill-creation` meta-skill + example + evals; run CI.
5. Generate `oscal-ssp-validate` and `fedramp-poam-qc`; fix any validator feedback.
6. Build index; verify router behavior; tag `v0.1.0`.

That’s the plan. If you want, I can output starter `validate_skill.py`, `build_index.py`, and the full `skills/skill-creation/SKILL.md` next.
